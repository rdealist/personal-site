# 性能评估与优化

## 1. 困惑度(Perplexity)如何衡量语言模型的质量?

**定义**: 模型对测试数据的"困惑程度",数值越低表示模型越好。

**数学公式**:
```
PPL = exp(-1/N × Σ log P(w_i | context))
```

**直观理解**:
- PPL = 10 → 模型平均在10个候选词中"犹豫"
- PPL = 100 → 模型平均在100个候选词中"犹豫"

**解读**:

| PPL值 | 含义 |
|-------|------|
| 更低 | 模型预测更准确,更"确定" |
| 更高 | 模型预测更不确定,更"困惑" |

**局限性**:
- 不能反映生成质量(流畅≠有用)
- 不同tokenizer不可直接比较
- 对短文本可能失真

**典型值**: GPT-2在WikiText-103上PPL约20-30,GPT-3约15-20

---

## 2. BLEU、ROUGE等评估指标在评估生成文本时有什么局限性?

**指标概述**:

| 指标 | 计算方式 | 主要用途 |
|------|----------|----------|
| **BLEU** | 生成文本与参考的n-gram重叠 | 机器翻译 |
| **ROUGE** | 召回率导向的n-gram匹配 | 文本摘要 |

**共同局限性**:

| 局限 | 说明 |
|------|------|
| **语义盲区** | "汽车"和"轿车"被视为不匹配 |
| **单参考依赖** | 多种正确表达可能得分为0 |
| **无序性问题** | 词序不同但语义相同会被惩罚 |
| **与人类判断脱节** | 高分不一定意味着高质量 |

**示例**:
```
参考: "The cat sat on the mat"
生成: "A feline rested upon the rug"
BLEU得分: 很低 (但语义正确)
```

**改进方向**: BERTScore (语义相似度)、人类评估、GPT-based评估

---

## 3. 什么是基准测试(Benchmark),为什么MMLU、HellaSwag等测试如此重要?

**定义**: 标准化的测试集和评估方法,用于客观比较不同模型的能力。

**重要基准测试**:

| 基准 | 测试内容 | 规模 |
|------|----------|------|
| **MMLU** | 57学科知识问答 | 14K问题 |
| **HellaSwag** | 常识推理补全 | 10K样本 |
| **HumanEval** | 代码生成能力 | 164问题 |
| **GSM8K** | 数学推理 | 8.5K问题 |
| **TruthfulQA** | 真实性评估 | 817问题 |

**为什么重要**:
1. **可比性**: 苹果对苹果的公平比较
2. **可复现**: 任何人可验证结果
3. **进度追踪**: 量化技术进步
4. **能力细分**: 识别模型强弱项

**局限性**:
- 可能被"刷榜"过拟合
- 不能覆盖所有真实场景
- 静态测试集会逐渐失效

---

## 4. 如何理解模型的偏差(Bias)和方差(Variance)之间的权衡?

**定义**:

| 概念 | 含义 | 表现 |
|------|------|------|
| **偏差** | 模型假设与真实规律的差距 | 系统性错误,欠拟合 |
| **方差** | 模型对训练数据变化的敏感度 | 随数据波动,过拟合 |

**权衡关系**:
```
总误差 = 偏差² + 方差 + 不可约误差
```

**模型复杂度与偏差-方差**:

| 复杂度 | 偏差 | 方差 | 问题 |
|--------|------|------|------|
| 过低 | 高 | 低 | 欠拟合 |
| 适中 | 适中 | 适中 | 最优 |
| 过高 | 低 | 高 | 过拟合 |

**实践启示**:
- 验证集性能是关键指标
- 正则化可降低方差
- 增加模型复杂度可降低偏差
- 集成学习可同时降低两者

---

## 5. 交叉验证(Cross-Validation)如何帮助我们更准确地评估模型性能?

**问题**: 单一训练/测试划分可能因数据分布不同产生偏差。

**K折交叉验证流程**:
```
数据集 → 分成K份
第1次: 第1份测试,2-K份训练
第2次: 第2份测试,1+3-K份训练
...
第K次: 第K份测试,1-(K-1)份训练
最终性能 = K次结果的平均值
```

**优势**:

| 优势 | 说明 |
|------|------|
| **数据利用率高** | 每个样本都被测试过 |
| **评估更稳定** | 减少单次划分的偶然性 |
| **置信度估计** | 可计算性能的标准差 |

**常见设置**:
- K=5 或 K=10 最常用
- 分层采样保持类别比例
- 留一法 (K=N): 最大化训练数据,但计算昂贵

---

## 6. 超参数(Hyperparameter)和参数(Parameter)有什么本质区别?

**核心区别**:

| 维度 | 参数 | 超参数 |
|------|------|--------|
| **设定方式** | 训练过程中自动学习 | 训练前人工设定 |
| **优化方法** | 梯度下降等 | 网格搜索、贝叶斯优化等 |
| **示例** | 权重W、偏置b | 学习率、层数、Batch Size |

**常见超参数**:

| 类别 | 超参数 |
|------|--------|
| **训练相关** | 学习率、batch size、epoch数 |
| **架构相关** | 层数、隐藏维度、注意力头数 |
| **正则化** | dropout率、L2系数 |
| **优化器** | momentum、beta1、beta2 |

**调优方法**:
1. **网格搜索**: 穷举组合,计算昂贵
2. **随机搜索**: 随机采样,效率更高
3. **贝叶斯优化**: 基于历史结果智能探索
4. **自动化调参**: Optuna、Ray Tune等工具

---

## 7. 学习率(Learning Rate)如何影响模型训练的速度和质量?

**作用**: 控制每次参数更新的步长。

```
θ_new = θ_old - learning_rate × gradient
```

**学习率影响**:

| 学习率 | 现象 | 后果 |
|--------|------|------|
| 过大 | 损失震荡/发散 | 无法收敛 |
| 过小 | 收敛极慢 | 训练时间过长 |
| 适中 | 稳定下降 | 最优结果 |

**学习率调度策略**:

| 策略 | 说明 |
|------|------|
| **恒定** | 最简单,可能不最优 |
| **阶梯衰减** | 固定epoch后降低 |
| **余弦退火** | 按余弦曲线平滑衰减 |
| **Warmup** | 从小学习率逐渐升高再衰减 |

**经验值**: 常见起始学习率 1e-3 到 1e-5,具体取决于模型和任务

---

## 8. 为什么说"模型越大越好"这个观念并不总是正确的?

**大模型的问题**:

| 问题 | 说明 |
|------|------|
| **过拟合风险** | 参数过多可能记忆训练数据 |
| **计算成本** | 训练和推理成本指数增长 |
| **延迟问题** | 推理速度慢,难以实时应用 |
| **边际效益递减** | 参数翻倍,性能提升有限 |
| **部署困难** | 需要高端硬件 |

**反例证据**:
- **Phi-2 (2.7B)** 在某些任务上超越 Llama-2 (7B)
- **Mistral 7B** 性能接近 Llama-2 13B
- 关键因素: 数据质量、架构设计、训练方法

**Scaling Law的另一面**:
```
性能 ∝ log(计算量)
```
即: 10倍计算可能只换来常数级提升。

**正确观念**: 模型大小应匹配任务需求和资源约束。

---

## 9. 量化(Quantization)技术如何在保持性能的同时压缩模型大小?

**原理**: 降低参数的数值精度。

```
FP32 (32位浮点) → FP16/INT8/INT4 (16/8/4位)
```

**压缩效果**:

| 精度 | 模型大小 | 性能损失 |
|------|----------|----------|
| FP32 | 100% | 基准 |
| FP16 | 50% | 几乎无损 |
| INT8 | 25% | 轻微 |
| INT4 | 12.5% | 可接受 |

**量化方法**:

| 方法 | 说明 |
|------|------|
| **训练后量化 (PTQ)** | 训练完成后直接量化 |
| **量化感知训练 (QAT)** | 训练时模拟量化误差 |
| **混合精度** | 敏感层保持高精度 |

**工具**: GPTQ, AWQ, bitsandbytes, GGML/GGUF

**实际效果**: 70B模型从140GB → 35GB (INT4),可在消费级显卡运行

---

## 10. 什么是知识蒸馏(Knowledge Distillation),它如何让"小模型学习大模型"?

**核心思想**: 用大模型(教师)的输出来训练小模型(学生)。

```
教师模型 (大) → 软标签/知识 → 学生模型 (小)
```

**为什么有效**:

| 方面 | 硬标签 | 软标签(教师输出) |
|------|--------|------------------|
| 信息量 | 只有正确答案 | 包含类别间的相似性 |
| 学习难度 | 非0即1 | 平滑的概率分布 |
| 泛化能力 | 较弱 | 继承教师的泛化 |

**蒸馏损失函数**:
```
Loss = α × CE(student, hard_labels) + (1-α) × KL(student, teacher_soft)
```

**典型应用**:
- DistilBERT: BERT的60%参数,97%性能
- TinyBERT: 更小的BERT变体
- 大模型蒸馏到端侧模型

**蒸馏的形式**:
- 输出蒸馏: 学习最终输出分布
- 特征蒸馏: 学习中间层表示
- 注意力蒸馏: 学习注意力模式
