# 模型架构与组件

## 1. Encoder(编码器)和Decoder(解码器)在Transformer中分别负责什么任务?

**职责分工**:

| 组件 | 输入 | 输出 | 核心任务 |
|------|------|------|----------|
| **Encoder** | 源序列 | 上下文表示 | 理解、编码输入信息 |
| **Decoder** | 目标序列+编码表示 | 下一个token概率 | 生成输出序列 |

**架构选择**:

| 模型类型 | 结构 | 代表模型 | 适用任务 |
|----------|------|----------|----------|
| Encoder-only | 仅编码器 | BERT | 理解类(分类、NER) |
| Decoder-only | 仅解码器 | GPT | 生成类(文本生成) |
| Encoder-Decoder | 两者结合 | T5, BART | 序列到序列(翻译、摘要) |

**关键区别**: Encoder可双向看全文,Decoder只能看已生成的内容(因果掩码)。

---

## 2. 多头注意力(Multi-Head Attention)为什么比单一注意力机制更强大?

**结构**:
```
Multi-Head = Concat(head_1, head_2, ..., head_h) × W_o
每个 head_i = Attention(Q×W_i^Q, K×W_i^K, V×W_i^V)
```

**优势**:

| 优势 | 说明 |
|------|------|
| **多子空间学习** | 不同头关注不同类型的关系 |
| **并行计算** | 多头可同时计算 |
| **表达能力强** | 综合多种注意力模式 |

**实际观察**: 不同注意力头学到了不同模式
- 某些头关注语法依赖(主谓关系)
- 某些头关注位置邻近性
- 某些头关注语义相似性

**类比**: 多人从不同角度观察同一事物,汇总后理解更全面。

---

## 3. 什么是残差连接(Residual Connection),它如何解决深度网络的训练难题?

**定义**:
```
输出 = F(x) + x  (而非 输出 = F(x))
```

**解决的问题**:

| 问题 | 残差连接的作用 |
|------|----------------|
| **梯度消失** | 梯度可通过恒等映射直接回传 |
| **退化问题** | 深层网络至少能保持浅层性能 |
| **训练困难** | 学习残差比学习完整映射更容易 |

**直观理解**:
- 网络只需学习"需要改变多少"(残差),而非"完整输出是什么"
- 如果某层不需要,权重可趋向0,等价于跳过该层

**效果**: 使数百层网络的训练成为可能(如ResNet-152, Transformer)

---

## 4. 层归一化(Layer Normalization)在神经网络中起什么作用?

**计算方式**:
```
LayerNorm(x) = γ × (x - μ) / √(σ² + ε) + β
其中 μ, σ 是该层激活值的均值和标准差
```

**作用**:

| 作用 | 说明 |
|------|------|
| **稳定训练** | 防止激活值过大或过小 |
| **加速收敛** | 减少内部协变量偏移 |
| **正则化效果** | 轻微的正则化作用 |

**与Batch Normalization的区别**:
- BN: 跨样本归一化,依赖batch统计量
- LN: 单样本内归一化,不依赖batch

**在Transformer中**: 通常用于每个子层(注意力、FFN)之后,配合残差连接使用。

---

## 5. 位置编码(Positional Encoding)如何让AI理解词语的先后顺序?

**问题**: 自注意力机制是置换不变的——"猫吃鱼"和"鱼吃猫"的处理无差异。

**解决方案**: 给每个位置添加唯一的位置信号。

**原始Transformer的方法**(正弦编码):
```
PE(pos, 2i) = sin(pos / 10000^(2i/d))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d))
```

**现代方法**:

| 方法 | 特点 |
|------|------|
| **正弦编码** | 固定、可泛化到更长序列 |
| **可学习位置编码** | 数据驱动,需训练 |
| **RoPE** | 旋转位置编码,相对位置信息 |
| **ALiBi** | 注意力偏置,支持长度外推 |

**效果**: 位置编码加入后,模型能区分"A在B之前"vs"B在A之前"。

---

## 6. 什么是生成对抗网络(GAN),它的"对抗"体现在哪里?

**架构**:
```
生成器(G): 随机噪声 → 生成假样本
判别器(D): 样本 → 真/假判断
```

**对抗过程**:
```
G的目标: 最大化 D(G(z)) —— 生成更逼真的样本骗过D
D的目标: 最大化 D(x_real) + 最小化 D(G(z)) —— 更好地区分真假
```

**博弈达到纳什均衡时**: G生成的样本与真实数据无法区分。

**训练动态**:
1. 初期: G生成的图像很假,D轻松辨别
2. 中期: G逐渐改进,D被迫提升
3. 收敛: G生成逼真样本,D只能随机猜测(50%准确率)

**应用**: 图像生成、风格迁移、超分辨率、数据增强

---

## 7. 卷积神经网络(CNN)为什么特别擅长处理图像数据?

**核心特性**:

| 特性 | 含义 | 对图像的优势 |
|------|------|--------------|
| **局部连接** | 每个神经元只看局部区域 | 捕捉边缘、纹理等局部特征 |
| **权重共享** | 同一卷积核扫描整张图 | 参数高效,位置不变性 |
| **层级结构** | 浅层→深层 | 边缘→纹理→部件→物体 |
| **池化操作** | 降采样 | 平移不变性,降低计算量 |

**参数效率对比**:
- 全连接: 224×224×3 → 1000 = 1.5亿参数
- 卷积: 相同任务仅需数百万参数

**经典结构**: LeNet → AlexNet → VGG → ResNet → EfficientNet

---

## 8. 循环神经网络(RNN)和长短期记忆网络(LSTM)如何处理序列数据?

**RNN基本结构**:
```
h_t = tanh(W_hh × h_{t-1} + W_xh × x_t + b)
```
每个时刻的隐状态依赖前一时刻,形成"记忆"。

**RNN的问题**: 长序列中梯度消失/爆炸,难以捕捉长距离依赖。

**LSTM解决方案**: 引入门控机制

| 门 | 作用 |
|----|------|
| **遗忘门** | 决定丢弃多少旧记忆 |
| **输入门** | 决定写入多少新信息 |
| **输出门** | 决定输出多少当前状态 |

**细胞状态**: 信息高速公路,可长期保持信息流动。

**现状**: 在NLP领域已基本被Transformer取代,但在某些时序任务中仍有应用。

---

## 9. 混合专家模型(MoE)是什么,为什么它能提升大模型的效率?

**架构**:
```
输入 → 路由器 → 选择Top-K个专家 → 专家计算 → 加权合并输出
```

**核心思想**: 不是所有参数都参与每次计算。

**效率提升原理**:

| 维度 | 传统模型 | MoE |
|------|----------|-----|
| 总参数量 | P | N×P (N个专家) |
| 每次激活参数 | P | K×P (K<<N) |
| 计算成本 | 与总参数成正比 | 与激活参数成正比 |

**关键技术**:
- **稀疏激活**: 每次只激活部分专家
- **负载均衡**: 确保专家被均匀使用
- **专家专业化**: 不同专家学习不同子任务

**代表模型**: Mixtral 8×7B (总参47B,激活13B,效果接近70B密集模型)

---

## 10. Dropout技术如何通过"随机关闭神经元"来提升模型泛化能力?

**机制**:
```
训练时: 以概率p随机将神经元输出置0
推理时: 使用全部神经元,权重乘以(1-p)
```

**为什么有效**:

| 效应 | 解释 |
|------|------|
| **打破共适应** | 防止神经元过度依赖特定其他神经元 |
| **隐式集成** | 相当于训练2^n个子网络的集成 |
| **噪声注入** | 增加训练难度,提升鲁棒性 |
| **正则化** | 限制模型有效容量 |

**常见设置**:
- 隐藏层: p = 0.5
- 输入层: p = 0.2
- 卷积层: 通常不用或用更低比例

**变体**: DropConnect (关闭连接而非神经元), Spatial Dropout (关闭整个特征图)
