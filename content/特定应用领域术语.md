# 特定应用领域术语

## 1. 自然语言处理(NLP)中的命名实体识别(NER)是做什么的?

**定义**: 从文本中识别并分类具有特定意义的实体。

**实体类型**:

| 类型 | 缩写 | 示例 |
|------|------|------|
| 人名 | PER | 张三、马云 |
| 地名 | LOC | 北京、长江 |
| 组织 | ORG | 阿里巴巴、清华大学 |
| 时间 | TIME | 2024年、下周一 |
| 金额 | MONEY | 100万美元 |

**示例**:
```
输入: "马云在杭州创立了阿里巴巴"
输出: [马云/PER] 在 [杭州/LOC] 创立了 [阿里巴巴/ORG]
```

**应用场景**:
- 信息抽取
- 知识图谱构建
- 问答系统
- 智能客服

**技术演进**: 规则 → CRF → BiLSTM-CRF → BERT-based

---

## 2. 词性标注(POS Tagging)如何帮助AI理解句子结构?

**定义**: 为每个词标注其语法类别(词性)。

**常见词性标签**:

| 标签 | 词性 | 示例 |
|------|------|------|
| NN | 名词 | 猫、书 |
| VB | 动词 | 跑、吃 |
| JJ | 形容词 | 大、漂亮 |
| RB | 副词 | 很、非常 |
| IN | 介词 | 在、从 |

**示例**:
```
"小猫在桌子上睡觉"
→ 小猫/NN 在/IN 桌子/NN 上/IN 睡觉/VB
```

**帮助理解句子结构**:
1. **消歧**: "花/NN"(名词)vs"花/VB"(动词:花费)
2. **句法分析基础**: 确定主谓宾结构
3. **下游任务**: 信息抽取、机器翻译的基础

---

## 3. 语义分割(Semantic Segmentation)在计算机视觉中如何工作?

**定义**: 为图像中的每个像素分配一个类别标签。

**与其他任务的区别**:

| 任务 | 粒度 | 输出 |
|------|------|------|
| 图像分类 | 整图 | 单一类别 |
| 目标检测 | 物体框 | 边界框+类别 |
| 语义分割 | 像素级 | 每个像素的类别 |
| 实例分割 | 像素级 | 每个像素的类别+实例ID |

**工作流程**:
```
输入图像 → 编码器(提取特征) → 解码器(恢复分辨率) → 像素级分类
```

**典型架构**: FCN, U-Net, DeepLab, SegFormer

**应用**: 自动驾驶(道路/车辆/行人)、医学影像(器官/病灶)、遥感图像

---

## 4. 目标检测(Object Detection)与图像分类(Image Classification)有何不同?

**核心区别**:

| 维度 | 图像分类 | 目标检测 |
|------|----------|----------|
| **问题** | 图像是什么? | 图像中有什么、在哪里? |
| **输出** | 类别标签 | 边界框+类别+置信度 |
| **数量** | 单标签或多标签 | 多个目标 |
| **定位** | 无 | 有 |

**目标检测输出示例**:
```
[
  {box: [x1,y1,x2,y2], class: "猫", confidence: 0.95},
  {box: [x3,y3,x4,y4], class: "狗", confidence: 0.87}
]
```

**主流方法**:
- **两阶段**: R-CNN系列 (先提议区域,再分类)
- **单阶段**: YOLO, SSD (直接预测框和类别)

**评估指标**: mAP (mean Average Precision), IoU (交并比)

---

## 5. 什么是文本摘要(Text Summarization),抽取式和生成式方法各有什么优劣?

**定义**: 将长文本压缩为保留核心信息的短文本。

**两种方法对比**:

| 方面 | 抽取式 | 生成式 |
|------|--------|--------|
| **原理** | 选择原文中的关键句子 | 生成新的概括性文本 |
| **保真度** | 高,原文句子 | 可能引入错误 |
| **流畅度** | 可能不连贯 | 更自然流畅 |
| **压缩率** | 有限 | 可高度压缩 |
| **技术难度** | 较低 | 较高 |

**抽取式方法**: TextRank, LexRank, BERT-based选择器

**生成式方法**: Seq2Seq, Transformer, GPT/T5

**实际应用**: 新闻摘要、会议纪要、论文摘要、邮件概述

---

## 6. 情感分析(Sentiment Analysis)如何判断文本的情感倾向?

**定义**: 自动识别文本中表达的情感极性或情感类别。

**任务类型**:

| 类型 | 输出 | 示例 |
|------|------|------|
| 二分类 | 正面/负面 | 商品评价 |
| 三分类 | 正面/中性/负面 | 社交媒体分析 |
| 细粒度 | 1-5星评分 | 评分预测 |
| 方面级 | 各方面的情感 | "服务好但价格贵" |

**技术路线**:
1. **词典方法**: 情感词典匹配计分
2. **机器学习**: 特征工程+分类器
3. **深度学习**: CNN/LSTM/BERT

**挑战**:
- 讽刺/反语检测
- 隐式情感表达
- 多情感混合
- 领域适应性

---

## 7. 机器翻译中的BLEU分数真的能准确反映翻译质量吗?

**BLEU计算**: 基于n-gram精确度,衡量译文与参考译文的重叠程度。

**BLEU的局限性**:

| 局限 | 说明 |
|------|------|
| **语义盲区** | 同义词替换得分低 |
| **参考依赖** | 参考译文有限 |
| **顺序不敏感** | n-gram无法捕捉长距离语序 |
| **与人类判断相关性** | 中等相关,非完美 |

**示例**:
```
参考: "The cat is on the mat"
译文A: "The cat sits on the mat" → BLEU较低
译文B: "Mat the on is cat the" → BLEU可能不低
```

**改进方向**:
- **chrF**: 字符级F1分数
- **COMET**: 基于神经网络的评估
- **人类评估**: 仍是金标准

**结论**: BLEU是有用的参考,但不应作为唯一标准。

---

## 8. 语音识别(Speech Recognition)和语音合成(Text-to-Speech)的技术原理有何不同?

**方向对比**:

| 方面 | 语音识别 (ASR) | 语音合成 (TTS) |
|------|----------------|----------------|
| **方向** | 语音 → 文本 | 文本 → 语音 |
| **输入** | 音频波形 | 文本字符 |
| **输出** | 文字序列 | 音频波形 |

**语音识别流程**:
```
音频 → 特征提取(MFCC) → 声学模型 → 语言模型 → 文本
```

**语音合成流程**:
```
文本 → 文本分析 → 声学特征预测 → 声码器 → 波形
```

**核心技术**:

| 任务 | 传统方法 | 深度学习方法 |
|------|----------|--------------|
| ASR | HMM-GMM | Wav2Vec, Whisper |
| TTS | 拼接/参数合成 | Tacotron, VITS |

**共同挑战**: 噪声处理、多说话人、情感表达、实时性

---

## 9. 推荐系统中的协同过滤(Collaborative Filtering)是如何工作的?

**核心思想**: "物以类聚,人以群分"——相似用户喜欢相似物品。

**两种主要方法**:

**用户协同过滤 (User-based CF)**:
```
1. 找到与目标用户相似的用户群
2. 推荐这些相似用户喜欢但目标用户未接触的物品
```

**物品协同过滤 (Item-based CF)**:
```
1. 计算物品之间的相似度
2. 推荐与用户历史喜欢物品相似的其他物品
```

**相似度计算**:
- 余弦相似度
- 皮尔逊相关系数
- Jaccard相似度

**矩阵分解方法**: 将用户-物品矩阵分解为用户向量和物品向量的乘积。

**局限**: 冷启动问题(新用户/新物品无历史数据)

---

## 10. 强化学习中的Q-Learning和策略梯度方法有什么区别?

**核心区别**:

| 方面 | Q-Learning | 策略梯度 |
|------|------------|----------|
| **学习目标** | 价值函数 Q(s,a) | 策略函数 π(a|s) |
| **决策方式** | 选max Q的动作 | 直接采样动作 |
| **动作空间** | 离散为主 | 可处理连续动作 |
| **类型** | Value-based | Policy-based |

**Q-Learning**:
```
Q(s,a) ← Q(s,a) + α[r + γ·max Q(s',a') - Q(s,a)]
```
学习"状态-动作对"的价值,选择价值最高的动作。

**策略梯度**:
```
∇J(θ) = E[∇log π(a|s) · R]
```
直接优化策略参数,使高回报动作的概率增加。

**典型算法**:
- Q-Learning: DQN, Double DQN
- 策略梯度: REINFORCE, PPO, A3C

**结合方法**: Actor-Critic (演员-评论家) 同时学习策略和价值
