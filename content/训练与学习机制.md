# 训练与学习机制

## 1. 监督学习、无监督学习和强化学习的核心差异在哪里?

| 学习范式 | 数据形式 | 学习目标 | 典型应用 |
|----------|----------|----------|----------|
| **监督学习** | 带标签数据 (X, Y) | 学习输入到输出的映射 | 分类、回归、翻译 |
| **无监督学习** | 无标签数据 (X) | 发现数据内在结构 | 聚类、降维、异常检测 |
| **强化学习** | 环境交互反馈 | 最大化累积奖励 | 游戏AI、机器人控制 |

**核心差异**:
- 监督学习: "老师给答案"
- 无监督学习: "自己找规律"
- 强化学习: "试错中学习"

---

## 2. 什么是迁移学习(Transfer Learning),为什么它能让AI"举一反三"?

**定义**: 将在源任务/领域学到的知识应用到目标任务/领域。

**原理**:
```
源任务知识 → 迁移 → 目标任务 (减少所需数据和训练时间)
```

**为什么有效**:
1. **底层特征通用**: 图像识别中,边缘、纹理等底层特征可复用
2. **语言规律共享**: NLP中语法、语义知识可迁移到不同任务
3. **减少冷启动成本**: 不必从随机权重开始

**经典案例**: ImageNet预训练 → 医学图像诊断;GPT预训练 → 特定领域对话

---

## 3. 微调(Fine-tuning)与从零训练相比,为什么能节省大量资源?

**资源对比**:

| 方面 | 从零训练 | 微调 |
|------|----------|------|
| 数据需求 | 海量 (TB级) | 少量 (KB-GB) |
| 计算成本 | 数百万美元 | 数百-数千美元 |
| 时间 | 数周-数月 | 数小时-数天 |

**为什么节省**:
1. **继承预训练知识**: 语言理解、世界知识已具备
2. **只调整部分参数**: 可冻结底层,只训练顶层
3. **起点更优**: 从已收敛的好参数开始,而非随机初始化

**比喻**: 如同培养专科医生——先完成通识教育(预训练),再专攻特定领域(微调)。

---

## 4. 预训练(Pretraining)阶段AI在学习什么,这个过程为何如此耗时?

**学习内容**:
- **语言层面**: 语法规则、词汇搭配、语义关系
- **知识层面**: 事实知识、常识推理、领域信息
- **模式层面**: 文本结构、推理模式、任务范式

**耗时原因**:

| 因素 | 规模 |
|------|------|
| 数据量 | 数万亿token (整个互联网) |
| 参数量 | 数十亿-数万亿参数 |
| 计算复杂度 | O(n²) 注意力计算 |
| 迭代次数 | 数据需多次遍历 |

**GPT-4训练成本估算**: ~1亿美元,数月训练时间。

---

## 5. 反向传播(Backpropagation)算法是如何让神经网络"从错误中学习"的?

**核心流程**:
```
前向传播: 输入 → 层层计算 → 输出 → 计算损失
反向传播: 损失 → 链式法则 → 逐层计算梯度 → 更新权重
```

**链式法则**:
```
∂Loss/∂w = ∂Loss/∂output × ∂output/∂hidden × ∂hidden/∂w
```

**"从错误中学习"的含义**:
1. 损失函数量化"错误程度"
2. 梯度指示"每个参数对错误的贡献"
3. 沿梯度反方向调整参数 → 减少错误

**直观理解**: 像调音师通过听音判断每根弦需要往哪个方向调、调多少。

---

## 6. 什么是梯度下降(Gradient Descent),为什么它是训练AI模型的核心优化方法?

**定义**: 沿损失函数梯度的反方向迭代更新参数,以最小化损失。

**更新公式**:
```
θ_new = θ_old - learning_rate × ∇Loss(θ)
```

**变体**:

| 类型 | 特点 | 适用场景 |
|------|------|----------|
| BGD | 全量数据计算梯度 | 小数据集 |
| SGD | 单样本计算梯度 | 在线学习 |
| Mini-batch GD | 小批量计算梯度 | 实际训练首选 |

**为什么是核心方法**:
- 适用于任意可微分损失函数
- 可扩展到超大规模参数
- 有成熟的变体 (Adam, AdaGrad等) 解决各种问题

---

## 7. 过拟合(Overfitting)和欠拟合(Underfitting)是如何影响模型性能的?

**定义对比**:

| 现象 | 训练集表现 | 测试集表现 | 本质问题 |
|------|-----------|-----------|----------|
| **欠拟合** | 差 | 差 | 模型太简单,学不到规律 |
| **过拟合** | 优秀 | 差 | 模型太复杂,记住了噪声 |
| **理想** | 好 | 好 | 学到了真正的规律 |

**识别方法**: 观察训练/验证损失曲线
- 两者都高 → 欠拟合
- 训练低、验证高且差距增大 → 过拟合

**解决方案**:
- 欠拟合: 增加模型复杂度、更多特征、更长训练
- 过拟合: 正则化、Dropout、数据增强、早停

---

## 8. 正则化(Regularization)技术如何防止模型"死记硬背"训练数据?

**核心思想**: 在损失函数中加入惩罚项,限制模型复杂度。

**常见方法**:

| 方法 | 原理 | 效果 |
|------|------|------|
| **L1正则化** | 惩罚权重绝对值之和 | 产生稀疏权重,特征选择 |
| **L2正则化** | 惩罚权重平方和 | 权重趋向小值,平滑决策边界 |
| **Dropout** | 随机关闭神经元 | 防止神经元共适应 |
| **早停** | 验证损失不降时停止 | 防止过度训练 |

**损失函数加正则项**:
```
Total Loss = Original Loss + λ × Regularization Term
```

**比喻**: 像考试时禁止死记题目答案,迫使学生理解原理。

---

## 9. Epoch、Batch和Iteration之间有什么关系,它们如何影响训练效率?

**定义与关系**:

```
1 Epoch = 完整遍历一次训练数据集
1 Epoch = (数据集大小 / Batch Size) 个 Iteration
1 Iteration = 处理一个Batch并更新一次参数
```

**示例**: 10000样本,Batch Size=100
- 1 Epoch = 100 Iterations
- 训练10 Epochs = 1000 Iterations

**影响效率的因素**:

| 参数 | 增大效果 | 减小效果 |
|------|----------|----------|
| Batch Size | 训练更稳定,GPU利用率高 | 更多噪声,可能泛化更好 |
| Epoch数 | 学习更充分 | 防止过拟合 |

**实践建议**: Batch Size通常取32-512,Epoch数通过早停机制确定。

---

## 10. 为什么说"数据质量比数据数量更重要",这在训练中如何体现?

**质量问题的影响**:

| 质量问题 | 后果 |
|----------|------|
| 标注错误 | 模型学到错误模式 |
| 数据偏差 | 模型产生偏见 |
| 噪声数据 | 降低模型性能 |
| 重复数据 | 过拟合特定样本 |

**实证案例**:
- **LIMA论文**: 仅1000条高质量数据微调,效果媲美大规模数据
- **Phi系列模型**: 用精选"教科书级"数据训练,小模型超越大模型

**体现方式**:
1. 数据清洗比收集更多数据更能提升性能
2. 精心设计的小数据集 > 粗糙的大数据集
3. 数据多样性、代表性比纯粹数量更重要

**公式化理解**: `模型质量 ≈ f(数据质量 × log(数据数量))`
